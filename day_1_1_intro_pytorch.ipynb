{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86c8d7b2",
      "metadata": {
        "id": "86c8d7b2"
      },
      "source": [
        "\n",
        "# Introduction to PyTorch\n",
        "\n",
        "PyTorch is an open-source deep learning framework developed by Facebook's AI Research lab. It is popular for its ease of use, dynamic computational graphs, and strong GPU support. PyTorch is widely used in both academia and industry for various applications like natural language processing, computer vision, and reinforcement learning.\n",
        "\n",
        "In this notebook, we'll cover the basic concepts and operations in PyTorch that will help build the foundation for implementing neural networks and using pre-built models from PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53391652",
      "metadata": {
        "id": "53391652"
      },
      "source": [
        "\n",
        "## Tensors in PyTorch\n",
        "\n",
        "Tensors are the basic building blocks in PyTorch, similar to NumPy arrays but with the added advantage of GPU acceleration. In this section, we will learn how to create and manipulate tensors in PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5525b918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5525b918",
        "outputId": "d910e7a4-c03c-4422-e7cd-dcbeb5578be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "\n",
            "Random Tensor:\n",
            " tensor([[0.7218, 0.3572, 0.9995],\n",
            "        [0.0427, 0.5590, 0.3319]])\n",
            "\n",
            "Tensor Addition:\n",
            " tensor([[ 6.,  8.],\n",
            "        [10., 12.]])\n",
            "\n",
            "Reshaped Tensor:\n",
            " tensor([1., 2., 3., 4.])\n",
            "\n",
            "Broadcast Tensor:\n",
            " tensor([[2., 4.],\n",
            "        [4., 6.]])\n",
            "Tensor on CPU:\n",
            " tensor([[1.7695, 1.6661, 0.6373,  ..., 0.9061, 1.5843, 0.2765],\n",
            "        [1.0162, 0.2150, 1.4708,  ..., 0.2819, 1.9416, 1.7394],\n",
            "        [0.7891, 1.1084, 1.6542,  ..., 1.2855, 1.5199, 0.3721],\n",
            "        ...,\n",
            "        [0.6938, 0.9346, 1.2154,  ..., 0.0750, 0.7258, 1.2016],\n",
            "        [1.9199, 0.4928, 1.1403,  ..., 1.3637, 0.1972, 1.6581],\n",
            "        [0.6172, 0.4212, 1.4229,  ..., 0.9813, 0.2339, 1.0867]])\n",
            "Time taken on CPU: 0.304184 seconds\n",
            "\n",
            "Tensor on GPU:\n",
            " tensor([[1.7695, 1.6661, 0.6373,  ..., 0.9061, 1.5843, 0.2765],\n",
            "        [1.0162, 0.2150, 1.4708,  ..., 0.2819, 1.9416, 1.7394],\n",
            "        [0.7891, 1.1084, 1.6542,  ..., 1.2855, 1.5199, 0.3721],\n",
            "        ...,\n",
            "        [0.6938, 0.9346, 1.2154,  ..., 0.0750, 0.7258, 1.2016],\n",
            "        [1.9199, 0.4928, 1.1403,  ..., 1.3637, 0.1972, 1.6581],\n",
            "        [0.6172, 0.4212, 1.4229,  ..., 0.9813, 0.2339, 1.0867]],\n",
            "       device='cuda:0')\n",
            "Time taken on GPU: 0.000981 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import PyTorch library\n",
        "import torch, time\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print(\"Tensor:\\n\", x)\n",
        "\n",
        "# Create a random tensor\n",
        "random_tensor = torch.rand((2, 3))\n",
        "print(\"\\nRandom Tensor:\\n\", random_tensor)\n",
        "\n",
        "# Perform tensor addition\n",
        "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "z = x + y\n",
        "print(\"\\nTensor Addition:\\n\", z)\n",
        "\n",
        "# Reshaping a tensor\n",
        "reshaped_tensor = x.view(4)\n",
        "print(\"\\nReshaped Tensor:\\n\", reshaped_tensor)\n",
        "\n",
        "# Broadcasting in tensors\n",
        "broadcast_tensor = x + torch.tensor([1, 2])\n",
        "print(\"\\nBroadcast Tensor:\\n\", broadcast_tensor)\n",
        "\n",
        "# # Tensor operations on GPU (if available)\n",
        "# if torch.cuda.is_available():\n",
        "#     x_cuda = x.to('cuda')\n",
        "#     print(\"\\nTensor on GPU:\\n\", x_cuda)\n",
        "# else:\n",
        "#     print(\"\\nCUDA not available. Running on CPU.\")\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.rand((10000, 10000), dtype=torch.float32)\n",
        "\n",
        "# Perform tensor operations on CPU\n",
        "start_time_cpu = time.time()\n",
        "cpu_result = x + x\n",
        "cpu_time = time.time() - start_time_cpu\n",
        "print(\"Tensor on CPU:\\n\", cpu_result)\n",
        "print(f\"Time taken on CPU: {cpu_time:.6f} seconds\")\n",
        "\n",
        "# Perform tensor operations on GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    x_cuda = x.to('cuda')\n",
        "    start_time_gpu = time.time()\n",
        "    gpu_result = x_cuda + x_cuda\n",
        "    gpu_time = time.time() - start_time_gpu\n",
        "    print(\"\\nTensor on GPU:\\n\", gpu_result)\n",
        "    print(f\"Time taken on GPU: {gpu_time:.6f} seconds\")\n",
        "else:\n",
        "    print(\"\\nCUDA not available. Running only on CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9252ac47",
      "metadata": {
        "id": "9252ac47"
      },
      "source": [
        "\n",
        "## Autograd and Computational Graphs\n",
        "\n",
        "PyTorch provides automatic differentiation using a feature called `autograd`. It tracks all operations on tensors that have `requires_grad=True`, and builds a computational graph. During the backward pass, it computes the gradients of all tensor operations automatically.\n",
        "\n",
        "Let's see a simple example of how this works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9c730fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9c730fb",
        "outputId": "12239064-8a22-4188-af3a-f088d67cf7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient (dy/dx): tensor(4.)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a tensor with requires_grad=True to track computation\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define a simple function f(x) = x^2\n",
        "y = x**2\n",
        "\n",
        "# Compute the gradient of y with respect to x\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient (dy/dx)\n",
        "print(\"Gradient (dy/dx):\", x.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10d9c22d",
      "metadata": {
        "id": "10d9c22d"
      },
      "source": [
        "\n",
        "## Building a Simple Neural Network\n",
        "\n",
        "In PyTorch, neural networks are built by creating classes that inherit from `torch.nn.Module`. These classes represent different layers, activation functions, and forward passes. Let's see an example of building a simple neural network with one hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "678967e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "678967e2",
        "outputId": "dc02b638-6250-4c4e-d34b-2f27e3b41a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output:\n",
            " tensor([[0.3990]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network with one hidden layer\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 3)  # Input layer (2 features) to hidden layer (3 neurons)\n",
        "        self.relu = nn.ReLU()          # Activation function (ReLU)\n",
        "        self.layer2 = nn.Linear(3, 1)  # Hidden layer (3 neurons) to output layer (1 output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the neural network\n",
        "model = SimpleNN()\n",
        "\n",
        "# Sample input\n",
        "input_data = torch.tensor([[1.0, 2.0]])\n",
        "output = model(input_data)\n",
        "print(\"Network output:\\n\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9d53a6",
      "metadata": {
        "id": "7d9d53a6"
      },
      "source": [
        "\n",
        "## Optimizers and Loss Functions\n",
        "\n",
        "Once the model is built, we need to define how it learns. This is done by specifying a loss function and an optimizer. Common loss functions include Mean Squared Error (MSE) and Cross-Entropy Loss. For optimizers, we can use methods like Stochastic Gradient Descent (SGD) or Adam. PyTorch provides the `torch.optim` module to easily implement optimizers.\n",
        "\n",
        "Here's an example of using a loss function and an optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "64ac20f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ac20f7",
        "outputId": "cded8cf5-61de-41b8-b3ec-3e004ce9bf4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.15918532013893127\n",
            "Updated model parameters:\n",
            "Parameter containing:\n",
            "tensor([[-0.3942, -0.6374],\n",
            "        [-0.2481, -0.3214],\n",
            "        [ 0.3430, -0.2309]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1191,  0.1849, -0.3192], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.1630,  0.0298, -0.2697]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3910], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define loss function (Mean Squared Error)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define optimizer (Stochastic Gradient Descent)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Sample target output\n",
        "target = torch.tensor([[0.0]])\n",
        "\n",
        "# Compute loss\n",
        "loss = loss_fn(output, target)\n",
        "print(\"Loss:\", loss.item())\n",
        "\n",
        "# Backpropagation: compute gradients\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# Update weights\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Updated model parameters:\")\n",
        "for param in model.parameters():\n",
        "    print(param)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6525803",
      "metadata": {
        "id": "a6525803"
      },
      "source": [
        "\n",
        "## PyTorch Computational Graphs\n",
        "\n",
        "PyTorch uses **dynamic computational graphs** which means that the graph is generated on-the-fly as operations are executed. This is different from **static computational graphs** (like those used in TensorFlow 1.x), which require you to define the entire graph before running computations.\n",
        "\n",
        "### Dynamic vs Static Graphs\n",
        "\n",
        "In PyTorch, the graph is recreated after each iteration, making it more intuitive and easier to debug. You can use Pythonâ€™s native control flow (e.g., loops and conditionals) while defining neural networks, which allows for flexibility in experimentation.\n",
        "\n",
        "**Static graphs** (like in TensorFlow) need to be defined first, and then they are executed. This can make them less intuitive to work with.\n",
        "\n",
        "### Benefits for Neural Network Training\n",
        "\n",
        "With dynamic graphs, you can easily modify the network architecture during training. This allows for more flexibility when experimenting with new ideas or debugging errors.\n",
        "\n",
        "For example, you can change the structure of the graph on every iteration of training, which is highly useful for tasks like reinforcement learning or neural architecture search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159c414c",
      "metadata": {
        "id": "159c414c"
      },
      "source": [
        "\n",
        "## Datasets and Data Loaders\n",
        "\n",
        "Neural networks require large amounts of data for training. PyTorch provides two classes: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` to help manage and load data efficiently.\n",
        "\n",
        "### Working with Datasets\n",
        "\n",
        "The `Dataset` class represents a dataset, and its primary purpose is to return individual samples and their corresponding labels. PyTorch provides built-in datasets like `torchvision.datasets`, but you can also define your custom dataset by subclassing `Dataset`.\n",
        "\n",
        "### DataLoader for Batching\n",
        "\n",
        "The `DataLoader` wraps a dataset and provides an iterator that can split the dataset into batches, shuffle data, and load data in parallel using multiple workers.\n",
        "\n",
        "Let's see how you can create a custom dataset and use DataLoader to handle it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e472fd93",
      "metadata": {
        "id": "e472fd93"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_names = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_names[idx])\n",
        "        image = Image.open(img_path)\n",
        "        label = 1 if \"cat\" in img_path else 0  # Example label\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "image_dir = 'path/to/your/images'\n",
        "custom_dataset = CustomImageDataset(image_dir)\n",
        "\n",
        "# Use DataLoader to create batches of data\n",
        "data_loader = DataLoader(custom_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Iterate over the data loader\n",
        "for images, labels in data_loader:\n",
        "    print(f\"Batch of images: {images.shape}, Batch of labels: {labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69622415",
      "metadata": {
        "id": "69622415"
      },
      "source": [
        "\n",
        "## PyTorch Modules and `nn.Module`\n",
        "\n",
        "In PyTorch, every layer or neural network is implemented as a subclass of the `nn.Module` class. This class provides the structure for building complex networks and layers, and it is the base class for all neural networks in PyTorch.\n",
        "\n",
        "### What is `nn.Module`?\n",
        "\n",
        "`nn.Module` serves as a container for layers and methods. Layers in a neural network, such as fully connected layers (e.g., `nn.Linear`), convolutional layers (e.g., `nn.Conv2d`), and activation functions (e.g., `nn.ReLU`), are subclasses of `nn.Module`.\n",
        "\n",
        "### Basic Usage of `nn.Module`\n",
        "\n",
        "Let's create a simple neural network using the `nn.Module` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "248d7f29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "248d7f29",
        "outputId": "64cd1e8a-c060-4101-ff5c-d325cbdcdb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[-0.0886]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network using nn.Module\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 3)  # Fully connected layer (4 inputs to 3 outputs)\n",
        "        self.relu = nn.ReLU()       # ReLU activation function\n",
        "        self.fc2 = nn.Linear(3, 1)  # Fully connected layer (3 inputs to 1 output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = SimpleNN()\n",
        "\n",
        "# Sample input\n",
        "input_data = torch.rand(1, 4)  # Batch of size 1 with 4 features\n",
        "output = model(input_data)\n",
        "print(\"Model output:\", output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}